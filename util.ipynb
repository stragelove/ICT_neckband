{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b460ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────\n",
    "# 라이브러리 / 상수 정의\n",
    "# ────────────────────────────────────────────────────────\n",
    "from functools import partial          # 콜백에 인자 “선바인딩” 하는 유틸\n",
    "import queue                           # 스레드 간 안전한 FIFO 큐\n",
    "\n",
    "from loguru import logger              # 구조화 로깅 라이브러리\n",
    "import numpy as np                     # 수치 연산 / 버퍼 생성\n",
    "\n",
    "# Hailo SDK: HEF 모델 파일, 가상 장치(VDevice) 및 스트림 포맷 정의\n",
    "from hailo_platform import (\n",
    "    HEF, VDevice,\n",
    "    FormatType, HailoSchedulingAlgorithm,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────\n",
    "# HailoAsyncInference 클래스\n",
    "# ────────────────────────────────────────────────────────\n",
    "class HailoAsyncInference:\n",
    "    \"\"\"\n",
    "    Hailo HEF 모델을 **비동기** 방식으로 실행해 스루풋을 최대화하는 헬퍼.\n",
    "\n",
    "    ◼ 입력:\n",
    "        이미 Log-Mel 스펙트로그램 형태로 전처리된 NumPy 텐서\n",
    "        (예: (1, 64, 101, 1) UINT8 또는 (1, 64, 101, 1) FLOAT32)\n",
    "\n",
    "    ◼ 출력:\n",
    "        • 단일 출력 → NumPy 배열  \n",
    "        • 다중 출력 → {출력_이름: NumPy 배열} 딕셔너리\n",
    "\n",
    "    ◼ 특징:\n",
    "        1) **Round-Robin 스케줄러**로 여러 작업을 HW 파이프라인에 고르게 배치\n",
    "        2) **큐 기반 Producer/Consumer** 구조 — I/O·전처리를 별도 스레드에서 실행 가능\n",
    "        3) SDK 가 제공하는 **run_async + callback** 패턴 사용\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) 초기화 --------------------------------------------------------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        hef_path,              # HEF 파일 경로 (컴파일된 Hailo 모델)\n",
    "        input_queue,           # Producer 가 채우는 입력 텐서 큐\n",
    "        output_queue,          # Consumer 가 읽을 결과 큐\n",
    "        batch_size=1,          # 모델 추론 배치 크기(K=1 권장: 스트리밍 지연 최소화)\n",
    "        input_type=None,       # 입력 스트림 타입(예: 'UINT8') — None 이면 HEF 메타 그대로\n",
    "        output_type=None,      # {\"out0\": \"UINT8\", \"out1\": \"FLOAT32\"} 식 지정\n",
    "        send_original_tensor=False  # 원본 텐서를 결과와 함께 저장(디버깅용)\n",
    "    ):\n",
    "        # 큐 핸들 저장\n",
    "        self.input_queue = input_queue\n",
    "        self.output_queue = output_queue\n",
    "\n",
    "        # VDevice 생성 파라미터 — 스케줄러를 Round-Robin 으로 명시\n",
    "        params = VDevice.create_params()\n",
    "        params.scheduling_algorithm = HailoSchedulingAlgorithm.ROUND_ROBIN\n",
    "\n",
    "        # HEF 로딩 후 가상 디바이스(VDevice) 인스턴스화\n",
    "        self.hef = HEF(hef_path)\n",
    "        self.target = VDevice(params)      # 실제 Hailo-8 장치를 추상화한 컨텍스트\n",
    "\n",
    "        # HEF 에서 추론 모델(인터프리터) 생성\n",
    "        self.infer_model = self.target.create_infer_model(hef_path)\n",
    "        self.infer_model.set_batch_size(batch_size)  # 배치 사이즈 반영\n",
    "\n",
    "        # 사용자가 타입을 오버라이드 했다면 VStream 포맷 변경\n",
    "        if input_type:\n",
    "            self._set_input_type(input_type)\n",
    "        if output_type:\n",
    "            self._set_output_type(output_type)\n",
    "\n",
    "        # 기타 옵션 저장\n",
    "        self.output_type = output_type\n",
    "        self.send_original_tensor = send_original_tensor\n",
    "\n",
    "    # 2) VStream 형식 설정 ---------------------------------------------------\n",
    "    def _set_input_type(self, input_type):\n",
    "        \"\"\"\n",
    "        모델에 입력 스트림이 1개뿐이라는 전제를 두고,  \n",
    "        모든 입력의 FormatType 을 동일 값으로 덮어쓴다.\n",
    "        \"\"\"\n",
    "        self.infer_model.input().set_format_type(getattr(FormatType, input_type))\n",
    "\n",
    "    def _set_output_type(self, output_type_dict):\n",
    "        \"\"\"\n",
    "        output_type_dict 예시:\n",
    "            {\"softmax\": \"UINT8\"}  또는 {\"cls\": \"FLOAT32\", \"score\": \"UINT8\"}\n",
    "        각 출력의 FormatType 을 개별적으로 설정한다.\n",
    "        \"\"\"\n",
    "        for name, fmt in output_type_dict.items():\n",
    "            self.infer_model.output(name).set_format_type(getattr(FormatType, fmt))\n",
    "\n",
    "    # 3) 콜백 ---------------------------------------------------------------\n",
    "    def callback(self, completion_info, bindings_list, batch_tensors):\n",
    "        \"\"\"\n",
    "        run_async 작업이 완료될 때 SDK 가 호출하는 함수.\n",
    "\n",
    "        • completion_info: 성공 여부 및 예외 정보\n",
    "        • bindings_list  : run_async 에 전달했던 바인딩 객체 리스트\n",
    "        • batch_tensors  : 해당 배치의 입력(디버깅용으로 큐에 다시 넘김)\n",
    "        \"\"\"\n",
    "        if completion_info.exception:\n",
    "            logger.error(f\"Inference error: {completion_info.exception}\")\n",
    "            return  # 예외 발생 시 결과 무시\n",
    "\n",
    "        # 각 바인딩마다 출력 버퍼 추출\n",
    "        for i, bindings in enumerate(bindings_list):\n",
    "            if len(bindings._output_names) == 1:    # 단일 출력\n",
    "                result = bindings.output().get_buffer()\n",
    "            else:                                   # 다중 출력\n",
    "                result = {\n",
    "                    n: np.expand_dims(bindings.output(n).get_buffer(), 0)\n",
    "                    for n in bindings._output_names\n",
    "                }\n",
    "            # (입력, 출력) 튜플을 출력 큐에 push — Consumer 측 후처리용\n",
    "            self.output_queue.put((batch_tensors[i], result))\n",
    "\n",
    "    # 4) 정보 조회 -----------------------------------------------------------\n",
    "    def get_vstream_info(self):\n",
    "        \"\"\"(입력 VStream 리스트, 출력 VStream 리스트) 반환.\"\"\"\n",
    "        return self.hef.get_input_vstream_infos(), self.hef.get_output_vstream_infos()\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        \"\"\"모델 첫 입력 VStream 의 텐서 shape 반환.\"\"\"\n",
    "        return self.hef.get_input_vstream_infos()[0].shape\n",
    "\n",
    "    # 5) 메인 루프 -----------------------------------------------------------\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        • `configure()` 컨텍스트 진입 → VStreams 실체화  \n",
    "        • 입력 큐에서 배치를 꺼내 run_async 호출  \n",
    "        • sentinel(None) 을 만나면 루프 종료 후 마지막 job.wait()\n",
    "        \"\"\"\n",
    "        with self.infer_model.configure() as cfg:\n",
    "            while True:\n",
    "                batch_data = self.input_queue.get()         # 큐에서 다음 배치 pop\n",
    "                if batch_data is None:                      # None = 종료 시그널\n",
    "                    break\n",
    "\n",
    "                # 원본 텐서 보존 여부에 따라 구조 결정\n",
    "                tensors = batch_data if not self.send_original_tensor else batch_data[1]\n",
    "\n",
    "                # 각 텐서마다 바인딩 객체 생성\n",
    "                bindings_list = []\n",
    "                for t in tensors:\n",
    "                    b = self._create_bindings(cfg)          # 출력 버퍼 구성\n",
    "                    b.input().set_buffer(np.asarray(t))     # 입력 버퍼 지정\n",
    "                    bindings_list.append(b)\n",
    "\n",
    "                # 디바이스가 Async 실행 가능할 때까지 대기\n",
    "                cfg.wait_for_async_ready(timeout_ms=10_000)\n",
    "\n",
    "                # 비동기 실행 → 완료 시 self.callback 호출\n",
    "                job = cfg.run_async(\n",
    "                    bindings_list,\n",
    "                    partial(\n",
    "                        self.callback,\n",
    "                        batch_tensors=(\n",
    "                            batch_data[0] if self.send_original_tensor else tensors\n",
    "                        ),\n",
    "                        bindings_list=bindings_list,\n",
    "                    ),\n",
    "                )\n",
    "            # 마지막 비동기 잡이 완전히 끝날 때까지 블록\n",
    "            job.wait(10_000)\n",
    "\n",
    "    # 6) 내부 유틸 -----------------------------------------------------------\n",
    "    def _dtype_str(self, info):\n",
    "        \"\"\"\n",
    "        VStream 의 FormatType → NumPy dtype 문자열(e.g. 'uint8') 로 변환.\n",
    "        output_type 사전이 없으면 HEF 메타데이터를 그대로 사용.\n",
    "        \"\"\"\n",
    "        if self.output_type is None:\n",
    "            return str(info.format.type).split(\".\")[1].lower()\n",
    "        return self.output_type[info.name].lower()\n",
    "\n",
    "    def _create_bindings(self, cfg):\n",
    "        \"\"\"\n",
    "        • 출력 버퍼(NumPy 배열) 사전 준비\n",
    "        • cfg.create_bindings() 로 바인딩 객체 생성\n",
    "        \"\"\"\n",
    "        if self.output_type is None:\n",
    "            # HEF 에 정의된 출력 스트림 형식(먼저 추론) 사용\n",
    "            out_bufs = {\n",
    "                info.name: np.empty(\n",
    "                    self.infer_model.output(info.name).shape,     # 출력 shape\n",
    "                    dtype=getattr(np, self._dtype_str(info)),      # np.uint8 등\n",
    "                )\n",
    "                for info in self.hef.get_output_vstream_infos()\n",
    "            }\n",
    "        else:\n",
    "            # 사용자가 dtype 을 명시했을 때\n",
    "            out_bufs = {\n",
    "                n: np.empty(\n",
    "                    self.infer_model.output(n).shape,\n",
    "                    dtype=getattr(np, dt.lower()),\n",
    "                )\n",
    "                for n, dt in self.output_type.items()\n",
    "            }\n",
    "        return cfg.create_bindings(output_buffers=out_bufs)\n",
    "\n",
    "# ────────────────────────────────────────────────────────\n",
    "# Log-Mel 전용 유틸리티\n",
    "# ────────────────────────────────────────────────────────\n",
    "def validate_logmels(tensors, batch_size):\n",
    "    \"\"\"\n",
    "    입력 텐서 리스트와 배치 크기 유효성 검증.\n",
    "\n",
    "    • 텐서가 0개면 ValueError  \n",
    "    • 텐서 수 % batch_size != 0 이면 ValueError\n",
    "    \"\"\"\n",
    "    if not tensors:\n",
    "        raise ValueError(\"Log-Mel 텐서 리스트가 비어 있습니다.\")\n",
    "    if len(tensors) % batch_size != 0:\n",
    "        raise ValueError(\"배치 크기로 나누어떨어지지 않습니다.\")\n",
    "\n",
    "def divide_to_batches(tensors, batch_size):\n",
    "    \"\"\"\n",
    "    제너레이터 패턴으로 배치 분할.\n",
    "    메모리 복사 없이 리스트 슬라이스 view 를 반환하므로 가벼움.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(tensors), batch_size):\n",
    "        yield tensors[i : i + batch_size]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

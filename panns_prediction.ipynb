{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91507bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1cf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./result/best.pt\"\n",
    "CLASS_NAME = [\"danger\", \"fire\", \"gas\", \"non\", \"tsunami\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 1.0\n",
    "\n",
    "sample_audio_path = \"./sample/sample.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(audio_path, sample_rate=SAMPLE_RATE, duration=DURATION):\n",
    "    '''\n",
    "    오디오 파일을 로드하고 전처리\n",
    "\n",
    "    Returns:\n",
    "        waveform: [1, data_length]\n",
    "    '''\n",
    "    num_samples = int(sample_rate * duration)\n",
    "\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "    except Exception as e:\n",
    "        print(f\"파일로드 오류 {audio_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    if waveform.shape[1] < num_samples:\n",
    "        waveform = F.pad(waveform, (0, num_samples  - waveform.shape[1]))\n",
    "        \n",
    "    else:\n",
    "        waveform = waveform[:, :num_samples]\n",
    "\n",
    "    # 절대값 정규화\n",
    "    waveform = waveform / (waveform.abs().max() + 1e-9)\n",
    "\n",
    "    return waveform\n",
    "\n",
    "def predict(model, audio_tensor, device=DEVICE, class_names=CLASS_NAME):\n",
    "    '''\n",
    "    전처리된 오디오 텐서에 대해 예측 수행.\n",
    "    \n",
    "    Args:\n",
    "        model: 학습된 PyTorch 모델\n",
    "        audio_tensor: [1, data_length] 형태의 오디오 텐서\n",
    "        device: 사용할 장치 (CPU 또는 CUDA)\n",
    "        class_names: 클래스 이름 리스트\n",
    "\n",
    "    Returns:\n",
    "        pred_labels: 예측된 클래스 이름 리스트\n",
    "        prob_np: 각 클래스별 확률 (NumPy 배열)\n",
    "    '''\n",
    "    audio_tensor = audio_tensor.to(device)\n",
    "\n",
    "    # 역전파 없음\n",
    "    with torch.no_grad():\n",
    "        output_dict = model(audio_tensor)\n",
    "        prob = output_dict[\"clipwise_output\"]\n",
    "        prob_np = prob.cpu().numpy()[0]\n",
    "\n",
    "        pred_tensor = (prob_np > 0.5).astype(int)\n",
    "        pred_labels = [class_names[i] for i, p in enumerate(pred_tensor) if p == 1]\n",
    "\n",
    "        # 모든 클래스가 임계값을 넘지 못했을때 가장 확률이 큰 클래스 반환\n",
    "        if not pred_labels:\n",
    "            highest_prob_idx = int(np.argmax(prob_np))\n",
    "            return [class_names[highest_prob_idx]], prob_np\n",
    "\n",
    "        return pred_labels, prob_np     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc39abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cnn14(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 257, kernel_size=(512,), stride=(160,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 257, kernel_size=(512,), stride=(160,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_block1): ConvBlock(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block2): ConvBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block3): ConvBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block4): ConvBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block5): ConvBlock(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block6): ConvBlock(\n",
       "    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (fc_audioset): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH, weights_only=False)\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72236e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 크기: torch.Size([1, 16000])\n"
     ]
    }
   ],
   "source": [
    "audio_tensor = preprocess(sample_audio_path)\n",
    "print(\"입력 텐서 크기:\", audio_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43c9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과: fire\n",
      "\n",
      "클래스별 확률:\n",
      " - danger: 0.0000\n",
      " - fire: 1.0000\n",
      " - gas: 0.0000\n",
      " - non: 0.0000\n",
      " - tsunami: 0.0000\n"
     ]
    }
   ],
   "source": [
    "pred_labels, class_prob = predict(model, audio_tensor)\n",
    "\n",
    "if pred_labels:\n",
    "    print(f\"예측결과: {\",\".join(pred_labels)}\")\n",
    "    \n",
    "print(\"\\n클래스별 확률:\")\n",
    "for idx, class_name in enumerate(CLASS_NAME):\n",
    "    print(f\" - {class_name}: {class_prob[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2.6.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

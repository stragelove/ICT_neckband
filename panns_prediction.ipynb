{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91507bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a1cf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./result/best.pt\"\n",
    "CLASS_NAME = [\"danger\", \"fire\", \"gas\", \"non\", \"tsunami\"] # 분류할 클래스\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 1.0\n",
    "NUM_SAMPLE = int(SAMPLE_RATE * DURATION)\n",
    "\n",
    "sample_audio_path = \"./sample/sample.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath, target_sample_rate=SAMPLE_RATE, target_duration=DURATION):\n",
    "    '''\n",
    "    오디오 파일을 로드하고 전처리\n",
    "    '''\n",
    "    num_samples_target = int(target_sample_rate * target_duration)\n",
    "\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"파일로드 오류 {filepath}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    if sr != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < num_samples_target:\n",
    "        waveform = F.pad(waveform, (0, num_samples_target  - current_length))\n",
    "        waveform = waveform[:, :num_samples_target]\n",
    "\n",
    "    waveform = waveform / (waveform.abs().max() + 1e-9)\n",
    "    waveform = waveform.squeeze(0)\n",
    "\n",
    "    return waveform\n",
    "\n",
    "def predict(model, audio_tensor, device=DEVICE, class_names=CLASS_NAME):\n",
    "    '''\n",
    "    전처리된 오디오에 대해 예측수행\n",
    "    '''\n",
    "    model.eval()\n",
    "    audio_tensor = audio_tensor.to(device)\n",
    "\n",
    "    if audio_tensor.ndim == 1:\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_dict = model(audio_tensor)\n",
    "        clipwise_output = output_dict['clipwise_output']\n",
    "\n",
    "        probabilities = clipwise_output\n",
    "        threshold = 0.5 # PANNs CNN14 모델의 기본값\n",
    "        predictions_tensor = (probabilities > threshold).int().cpu().numpy()\n",
    "        predicted_labels = []\n",
    "\n",
    "        for i, class_prediction in enumerate(predictions_tensor[0]):\n",
    "            if class_prediction == 1:\n",
    "                predicted_labels.append(class_names[i])\n",
    "\n",
    "        if not predicted_labels:\n",
    "            highest_prob_idx = torch.argmax(probabilities, dim=1).item()\n",
    "            return [f\"{class_names[highest_prob_idx]}\"], probabilities.cpu().numpy()[0]\n",
    "\n",
    "        return predicted_labels, probabilities.cpu().numpy()[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc39abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cnn14(\n",
       "  (spectrogram_extractor): Spectrogram(\n",
       "    (stft): STFT(\n",
       "      (conv_real): Conv1d(1, 257, kernel_size=(512,), stride=(160,), bias=False)\n",
       "      (conv_imag): Conv1d(1, 257, kernel_size=(512,), stride=(160,), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (logmel_extractor): LogmelFilterBank()\n",
       "  (spec_augmenter): SpecAugmentation(\n",
       "    (time_dropper): DropStripes()\n",
       "    (freq_dropper): DropStripes()\n",
       "  )\n",
       "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_block1): ConvBlock(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block2): ConvBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block3): ConvBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block4): ConvBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block5): ConvBlock(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_block6): ConvBlock(\n",
       "    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (fc_audioset): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH, weights_only=False)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72236e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16000])\n"
     ]
    }
   ],
   "source": [
    "audio_tensor = preprocess(sample_audio_path)\n",
    "print(audio_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b43c9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측결과: ['fire']\n",
      "\n",
      "클래스별 확률:\n",
      " - danger: 0.0000\n",
      " - fire: 0.9999\n",
      " - gas: 0.0002\n",
      " - non: 0.0000\n",
      " - tsunami: 0.0019\n"
     ]
    }
   ],
   "source": [
    "predicted_labels, class_probabilities = predict(model, audio_tensor)\n",
    "\n",
    "if predicted_labels:\n",
    "    # print(f\"예측결과: {\",\".join(predicted_labels)}\")\n",
    "    print(f\"예측결과: {predicted_labels}\")\n",
    "print(\"\\n클래스별 확률:\")\n",
    "for i, class_name_val in enumerate(CLASS_NAME):\n",
    "    print(f\" - {class_name_val}: {class_probabilities[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2.6.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
